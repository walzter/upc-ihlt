{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                  Semantic Textual Similarity (STS) Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to be able to determine the similarity between two sentences. One sentence is said to be \"parraphrased\" when the content (or message) is the same, but uses different words and or structure. \n",
    "\n",
    "An example from the trial set: \n",
    " - The bird is bathing in the sink.\n",
    "\n",
    " - Birdie is washing itself in the water basin.\n",
    "\n",
    "Here we are given a set of training and testing sets in which they are labeled with the \"gs\", on a scale of 0-5. \n",
    "\n",
    "|label|\tdescription|\n",
    "| :-: | :-: |\n",
    "|5\t| They are completely equivalent, as they mean the same thing.|\n",
    "|4\t| They are mostly equivalent, but some unimportant details differ.|\n",
    "|3\t| They are roughly equivalent, but some important information differs/missing.|\n",
    "|2\t| They are not equivalent, but share some details.|\n",
    "|1\t| They are not equivalent, but are on the same topic.|\n",
    "|0\t| They are on different topics.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Loader file with two functions: load_sentences \n",
    "from helper_funcs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our data loaders, data preprocessing, feature extraction and post-processing are stored in the helper_funcs.py file. \n",
    "\n",
    "This was done such that we would have a cleaner notebook to present. \n",
    "\n",
    "Below, we can see the initial steps of our pipeline: \n",
    "\n",
    "**Functions:**\n",
    "\n",
    "- *load_sentences(PATH):* it will go to the specified PATH, open and read all files which have STS.input.*. This is loaded into a pandas.DataFrame for easier data manipulation. \n",
    "- *load_gs(PATH):* similar to load_sentences, it will go to a specified PATH and load all STS.gs.* files into a pandas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# TRAINING PATH\n",
    "TRAIN_PATH = './data/train/input/'\n",
    "TRAIN_GS_PATH = './data/train/gs/'\n",
    "# TEST PATH\n",
    "TEST_PATH = 'data/test/input/'\n",
    "TEST_GS_PATH = './data/test/gs/'\n",
    "\n",
    "# Loading the Data \n",
    "# --> COMMENT THESE LINES IF FILES ARE ALREADY PICKLED\n",
    "X_train, y_train, X_test, y_test = load_sentences(TRAIN_PATH), load_gs(TRAIN_GS_PATH),load_sentences(TEST_PATH), load_gs(TEST_GS_PATH)\n",
    "\n",
    "# X_train with extracted features and standardized values \n",
    "#X_train_scaled_norm = extract_features(X_train,scaled=True)\n",
    "# X_test with extracted features and standardized values \n",
    "#X_test_scaled_norm = extract_features(X_test,scaled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the files for later use if necesary as to not calculate more\n",
    "# saving the training features \n",
    "with open(\"./X_train_scaled_norm.pickle\",'wb') as f:\n",
    "    pickle.dump(X_train_scaled_norm,f,pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# saving the testing features \n",
    "with open(\"./X_test_scaled_norm.pickle\",'wb') as f:\n",
    "    pickle.dump(X_test_scaled_norm,f,pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the pickle files to avoid re-processing \n",
    "X_train_scaled_norm = pickle.load( open( \"X_train_scaled_norm.pickle\", \"rb\" ) )\n",
    "X_test_scaled_norm = pickle.load( open( \"X_test_scaled_norm.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEARSON CORRELATION 0.7892\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from scipy.stats import pearsonr\n",
    "svr = SVR(kernel = 'rbf', gamma = 0.02, C = 150, epsilon = 0.50, tol = 0.1)\n",
    "svr.fit(X_train_scaled_norm, y_train.values.reshape(-1,))\n",
    "\n",
    "# Predict\n",
    "y_pred = svr.predict(X_test_scaled_norm)\n",
    "\n",
    "y_c, y_d = y_pred.reshape(-1,), y_test.values.reshape(-1,)\n",
    "\n",
    "corr = pearsonr(y_c, y_d)[0]\n",
    "#0.7892\n",
    "print(f\"PEARSON CORRELATION {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between our model and baseline: 0.0330\n",
      "The percent change was 4.3588%\n"
     ]
    }
   ],
   "source": [
    "# checking the difference between the baseline \n",
    "baseline = 0.7562 \n",
    "diff = corr - baseline\n",
    "pcnt_chng = ((corr-baseline) / (baseline) )*100\n",
    "print(f\"Difference between our model and baseline: {diff:.4f}\")\n",
    "print(f\"The percent change was {pcnt_chng:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an MLP\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "HIDDEN_LAYERS = (500,3)\n",
    "BATCH_SIZE = (250)\n",
    "\n",
    "mlp_regr = MLPRegressor(hidden_layer_sizes=HIDDEN_LAYERS,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        early_stopping=False,\n",
    "                        solver='sgd',\n",
    "                        max_iter=5000,\n",
    "                        epsilon=1e-7,\n",
    "                        learning_rate_init=0.01,\n",
    "                        learning_rate='adaptive',\n",
    "                        verbose=True,\n",
    "                        n_iter_no_change=30).fit(X_train_scaled_norm,y_train.values.reshape(-1,))\n",
    "\n",
    "y_pred = mlp_regr.predict(X_test_scaled_norm)\n",
    "\n",
    "score = mlp_regr.score(X_test_scaled_norm, y_test.values.reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = pearsonr(y_pred, y_test)[0]\n",
    "print(f\"PEARSON CORRELATION {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7b0227df9456bca28f69aef5d39629306c490f3d5d6e0ff9d2fd6f7d7f6a539"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
