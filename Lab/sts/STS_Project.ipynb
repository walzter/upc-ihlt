{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "# TODO = [] \n",
    "\n",
    "## FEATURE ENGINEERING\n",
    "## Lexical Similarities:\n",
    "# FuzzyWuzzy []\n",
    "\n",
    "## MODEL\n",
    "# SVM \n",
    "\n",
    "## Data Augmentation\n",
    "# Back Translation => English --> Another Language --> English \n",
    "\n",
    "## Final Eval\n",
    "# Pearson []\n",
    "# Confidence Interval? []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Textual Similarity (STS) Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to be able to determine the similarity between two sentences. One sentence is said to be \"parraphrased\" when the content (or message) is the same, but uses different words and or structure. \n",
    "\n",
    "An example from the trial set: \n",
    " - The bird is bathing in the sink.\n",
    "\n",
    " - Birdie is washing itself in the water basin.\n",
    "\n",
    "Here we are given a set of training and testing sets in which they are labeled with the \"gs\", on a scale of 0-5. \n",
    "\n",
    "|label|\tdescription|\n",
    "| :-: | :-: |\n",
    "|5\t| They are completely equivalent, as they mean the same thing.|\n",
    "|4\t| They are mostly equivalent, but some unimportant details differ.|\n",
    "|3\t| They are roughly equivalent, but some important information differs/missing.|\n",
    "|2\t| They are not equivalent, but share some details.|\n",
    "|1\t| They are not equivalent, but are on the same topic.|\n",
    "|0\t| They are on different topics.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the following: \n",
    "- Read in the sentences as a total dataframe  --> Either load all three dataframes and then append them into a bigger one. \n",
    "- append the corresponding GS to the dataframe  --> Add this one to the previous df \n",
    "- Create a utils file in which we have all the features we want to create\n",
    "- Show which features were created and how/why \n",
    "- we can then create a pipeline\n",
    "    - Takes in all the features from before and makes them into a feature array \n",
    "    - Standardizes the values \n",
    "    - Outputs a simple N-D array with all the processed / calculated features \n",
    "\n",
    "- We need to create 3 variations: \n",
    "    1. \"Standard\" distance similarities \n",
    "    2. \"XTRa Train\" --> With more training data doing back-translation and AEDA \n",
    "\n",
    "\n",
    "*STEPS:*\n",
    "1. Preprocess textual data \n",
    "    - Read in sentence pairs --> DONE\n",
    "    - Tokenize --> DONE\n",
    "    - Pos Tag ---> DONE\n",
    "    - Remove stopwords and punctuation  --> DONE\n",
    "\n",
    "2. Extract Features \n",
    "    - Similarity measures \n",
    "    - Word frequency \n",
    "    - Tf-IDF ?\n",
    "3. Generate Extra Data (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import sentiwordnet\n",
    "from nltk.corpus import wordnet_ic\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('wordnet_ic')\n",
    "# setting the wordnet_ic \n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Loader file with two functions: load_sentences \n",
    "from data_loader_ import *\n",
    "\n",
    "# Preprocessing file with several prerpocessing functions\n",
    "from pre_processing import *\n",
    "\n",
    "# feature extraction \n",
    "from feature_extractor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PATH\n",
    "TRAIN_PATH = './data/train/input/'\n",
    "TRAIN_GS_PATH = './data/train/gs/'\n",
    "\n",
    "# TEST PATH\n",
    "TEST_PATH = 'data/test/input/'\n",
    "TEST_GS_PATH = './data/test/gs/'\n",
    "\n",
    "# Loading the Data \n",
    "X_train, y_train, X_test, y_test = load_sentences(TRAIN_PATH), load_gs(TRAIN_GS_PATH),load_sentences(TEST_PATH), load_gs(TEST_GS_PATH)\n",
    "\n",
    "#sepparating the sentences \n",
    "SA, SB = get_processed_sentences(X_train)\n",
    "\n",
    "# Jaccard_Fuzzy_Lev\n",
    "feature_df = jd_fuzz_lev(SA, SB, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 2234 sentence pairs\n"
     ]
    }
   ],
   "source": [
    "# Lets look at how many total values we have\n",
    "print(f\"We have a total of {X_train.shape[0]} sentence pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical similarities\n",
    "# getting the synset similarity\n",
    "# we use the lemmas of SA and SB\n",
    "\n",
    "def get_similarities(word1, word2,brown_ic,ret_dict=None):\n",
    "\n",
    "  '''\n",
    "  input --> Word1, Word2\n",
    "  Output --> dict of similarities \n",
    "\n",
    "  '''\n",
    "  simil_dict = dict()\n",
    "\n",
    "  def path_sim(word1,word2): \n",
    "    return word1.path_similarity(word2)\n",
    "  def lch_sim(word1,word2): \n",
    "    return word1.lch_similarity(word2)\n",
    "  def wup_sim(word1,word2): \n",
    "    return word1.wup_similarity(word2)\n",
    "  def lin_sim(word1,word2,brown_ic): \n",
    "    '''\n",
    "    needs information content (IC) of LCS (least common subsumer)\n",
    "    '''\n",
    "    return word1.lin_similarity(word2,brown_ic)\n",
    "\n",
    "  simil_dict['PATH_SIMIL'] = path_sim(word1,word2)\n",
    "  simil_dict['LCH_SIMIL'] = lch_sim(word1,word2)\n",
    "  simil_dict['WUP_SIMIL'] = wup_sim(word1,word2)\n",
    "  simil_dict['LIN_SIMIL'] = lin_sim(word1,word2,brown_ic)\n",
    "  all_sims = [path_sim(word1,word2),lch_sim(word1,word2),wup_sim(word1,word2),lin_sim(word1,word2,brown_ic)]\n",
    "  if ret_dict==True:\n",
    "    return simil_dict\n",
    "  elif ret_dict==False:\n",
    "    return all_sims\n",
    "\n",
    "\n",
    "def get_max_sim_synset(lemmaA, lemmaB,brown_ic):\n",
    "  d = dict()\n",
    "  # getting the synsets: \n",
    "  syn1 = wordnet.synsets(lemmaA)\n",
    "  syn2 = wordnet.synsets(lemmaB)\n",
    "  for asynset in syn1:\n",
    "    for bsynset in syn2:  \n",
    "      sims = get_similarities(asynset, bsynset,brown_ic)\n",
    "      max_key = max(sims, key=sims.get)\n",
    "      d[(lemmaA,lemmaB,max_key)] = sims[max_key]\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sA = SA.SentA_lemmas[0]\n",
    "sB = SB.SentB_lemmas[0]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "d = dict()\n",
    "l = []\n",
    "for idx, x in enumerate(sA):\n",
    "    #print(x, wordnet.synsets(x))\n",
    "    syn1 = wordnet.synsets(x)\n",
    "    syn2 = wordnet.synsets(sB[idx])\n",
    "    for xx in syn1:\n",
    "        for yy in syn2:  \n",
    "            try:  \n",
    "                sims = get_similarities(xx, yy, brown_ic,ret_dict=False)\n",
    "                l.append(sims)\n",
    "            except:\n",
    "                continue\n",
    "            #max_val = max(sims, key=sims.get)\n",
    "            d[xx, yy] = sims\n",
    "            \n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = np.array(sum(l, [])).reshape(-1,1)\n",
    "scaler.fit(l1)\n",
    "vals = scaler.transform(l1)\n",
    "vals\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "scaler.fit(df.values)\n",
    "vals = scaler.transform(df.values)\n",
    "\n",
    "df1 = pd.DataFrame(vals)\n",
    "df1.columns = df.columns\n",
    "d = df1.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'a':5,'b':3,'c':8}\n",
    "k = max(d, key=d.get)\n",
    "print(k, d[k])\n",
    "\n",
    "# get key-value max\n",
    "b = max(d.items(), key=lambda x: x[1])\n",
    "print(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler1 = StandardScaler()\n",
    "scaler1.fit(feature_df.values)\n",
    "X_scaled = scaler1.transform(feature_df.values)\n",
    "\n",
    "SA1, SB1 = get_processed_sentences(X_test)\n",
    "feature_df_test = jd_fuzz_lev(SA1, SB1, X_test)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feature_df_test.values)\n",
    "X_scaled_test = scaler.transform(feature_df_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.svm import SVR\n",
    "from scipy.stats import pearsonr\n",
    "svr = SVR(kernel = 'rbf', gamma = 0.01, C = 200, epsilon = 0.50, tol = 0.25)\n",
    "svr.fit(X_scaled, y_train.values.reshape(-1,))\n",
    "\n",
    "# Predict\n",
    "test_predict = svr.predict(X_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation: 0.11811386277788924\n"
     ]
    }
   ],
   "source": [
    "correlation = pearsonr(test_predict, y_test.values.reshape(-1,))[0]\n",
    "print(\"Pearson correlation:\", correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    8.3s\n"
     ]
    }
   ],
   "source": [
    "#pass a gridsearch over this \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {'kernel' : ('linear', 'poly', 'rbf', 'sigmoid'),'C' : [1,5,10],'degree' : [3,8],'coef0' : [0.01,10,0.5],'gamma' : ('auto','scale')},\n",
    "\n",
    "modelsvr = SVR()\n",
    "\n",
    "grids = GridSearchCV(modelsvr,param,cv=5,n_jobs=-1,verbose=2)\n",
    "\n",
    "grids.fit(X_scaled, y_train.values.reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = grids.predict(X_scaled_test)\n",
    "correlation = pearsonr(test_predict, y_test.values.reshape(-1,))[0]\n",
    "print(\"Pearson correlation:\", correlation)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7b0227df9456bca28f69aef5d39629306c490f3d5d6e0ff9d2fd6f7d7f6a539"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
