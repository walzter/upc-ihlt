{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "# TODO = [] \n",
    "# TF-IDF [] -> Term Frequency - Inverse Data Frequency (TF-IDF)\n",
    "\n",
    "## FEATURE ENGINEERING\n",
    "# Lexical Similarities -> Levenstein Distance, Jaccard Distance []\n",
    "# FuzzyWuzzy []\n",
    "\n",
    "## MODEL\n",
    "# SVM \n",
    "# NN? CNN?\n",
    "\n",
    "## Data Augmentation\n",
    "# Back Translation => English --> Another Language --> English \n",
    "# AEDA\n",
    "\n",
    "\n",
    "## Final Eval\n",
    "# Pearson []\n",
    "# Confidence Interval? []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Textual Similarity (STS) Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to be able to determine the similarity between two sentences. One sentence is said to be \"parraphrased\" when the content (or message) is the same, but uses different words and or structure. \n",
    "\n",
    "An example from the trial set: \n",
    " - The bird is bathing in the sink.\n",
    "\n",
    " - Birdie is washing itself in the water basin.\n",
    "\n",
    "Here we are given a set of training and testing sets in which they are labeled with the \"gs\", on a scale of 0-5. \n",
    "\n",
    "|label|\tdescription|\n",
    "| :-: | :-: |\n",
    "|5\t| They are completely equivalent, as they mean the same thing.|\n",
    "|4\t| They are mostly equivalent, but some unimportant details differ.|\n",
    "|3\t| They are roughly equivalent, but some important information differs/missing.|\n",
    "|2\t| They are not equivalent, but share some details.|\n",
    "|1\t| They are not equivalent, but are on the same topic.|\n",
    "|0\t| They are on different topics.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the following: \n",
    "- Read in the sentences as a total dataframe  --> Either load all three dataframes and then append them into a bigger one. \n",
    "- append the corresponding GS to the dataframe  --> Add this one to the previous df \n",
    "- Create a utils file in which we have all the features we want to create\n",
    "- Show which features were created and how/why \n",
    "- we can then create a pipeline\n",
    "    - Takes in all the features from before and makes them into a feature array \n",
    "    - Standardizes the values \n",
    "    - Outputs a simple N-D array with all the processed / calculated features \n",
    "\n",
    "- We need to create 3 variations: \n",
    "    1. \"Standard\" distance similarities \n",
    "    2. \"XTRa Train\" --> With more training data doing back-translation and AEDA \n",
    "\n",
    "\n",
    "*STEPS:*\n",
    "1. Preprocess textual data \n",
    "    - Read in sentence pairs --> DONE\n",
    "    - Tokenize --> DONE\n",
    "    - Pos Tag ---> DONE\n",
    "    - Remove stopwords and punctuation  --> DONE\n",
    "\n",
    "2. Extract Features \n",
    "    - Similarity measures \n",
    "    - Word frequency \n",
    "    - Tf-IDF ?\n",
    "3. Generate Extra Data (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import sentiwordnet\n",
    "from nltk.corpus import wordnet_ic\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('wordnet_ic')\n",
    "# setting the wordnet_ic \n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Eric/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Loader file with two functions\n",
    "from data_loader_ import *\n",
    "\n",
    "# Preprocessing file with several prerpocessing functions\n",
    "from pre_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PATH\n",
    "TRAIN_PATH = './data/train/input/'\n",
    "TRAIN_GS_PATH = './data/train/gs/'\n",
    "\n",
    "# TEST PATH\n",
    "TEST_PATH = 'data/test/input/'\n",
    "TEST_GS_PATH = './data/test/gs/'\n",
    "\n",
    "# Loading the Data \n",
    "X_train, y_train, X_test, y_test = load_sentences(TRAIN_PATH), load_gs(TRAIN_GS_PATH),load_sentences(TEST_PATH), load_gs(TEST_GS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 2234 sentence pairs\n"
     ]
    }
   ],
   "source": [
    "# Lets look at how many total values we have\n",
    "print(f\"We have a total of {X_train.shape[0]} sentence pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man is riding a bicycle.\n",
      "['a', 'man', 'is', 'riding', 'a', 'bicycle']\n",
      "['man', 'riding', 'bicycle']\n",
      "[('man', 'NN'), ('riding', 'VBG'), ('bicycle', 'NN')]\n",
      "['man', 'rid', 'bicycle']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# choosing a random sentence and see if the pre-processing works \n",
    "#SEED = np.random.seed(42)\n",
    "#samples = np.array(X_train.SentA).reshape(-1,)\n",
    "#sample_sentence = np.random.choice(samples, 1)[0]\n",
    "\n",
    "sample_sentence = X_train.iloc[0,:].SentA\n",
    "sample_sentence2 = X_train.iloc[0,:].SentB\n",
    "# original sentence \n",
    "print(sample_sentence)\n",
    "# stripping the punctuation works \n",
    "print(strip_punctuation(sample_sentence))\n",
    "# stripping the stopwords works \n",
    "print(strip_stopwords_punctuation(sample_sentence))\n",
    "# saving the tokenized sentence \n",
    "toked_sent = strip_stopwords_punctuation(sample_sentence)\n",
    "toked_sent = clean_replace_unwanted_chars(toked_sent)\n",
    "# getting the POS-Tag\n",
    "print(get_pos_tag(toked_sent))\n",
    "#saving the pos_tagged sentence \n",
    "pos_tagged = get_pos_tag(toked_sent)\n",
    "# getting the lemmas \n",
    "toked_sent_lemm = get_lemmas(pos_tagged)\n",
    "print(toked_sent_lemm)\n",
    "\n",
    "\n",
    "def check_sent(sentA, sentB):\n",
    "    print(strip_punctuation(sentA))\n",
    "    print(strip_punctuation(sentB))\n",
    "    print(strip_stopwords_punctuation(sentA))\n",
    "    print(strip_stopwords_punctuation(sentB))\n",
    "    toked_sentA = strip_stopwords_punctuation(sentA)\n",
    "    toked_sentA = clean_replace_unwanted_chars(toked_sentA)\n",
    "    toked_sentB = strip_stopwords_punctuation(sentB)\n",
    "    toked_sentB = clean_replace_unwanted_chars(toked_sentB)\n",
    "    pos_taggedA = get_pos_tag(toked_sentA)\n",
    "    pos_taggedB = get_pos_tag(toked_sentB)\n",
    "    print(get_pos_tag(toked_sentA))\n",
    "    print(get_pos_tag(toked_sentB))\n",
    "    toked_sent_lemmA = get_lemmas(pos_taggedA)\n",
    "    toked_sent_lemmB = get_lemmas(pos_taggedB)\n",
    "    print(toked_sent_lemmA)\n",
    "    print(toked_sent_lemmB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1)-->Score syn-syn2\n",
      "Synset('bicycle.n.01') Synset('motorcycle.n.01') 0.7272727272727273\n",
      "(2)-->Score syn2-syn\n",
      "Synset('motorcycle.n.01') Synset('bicycle.n.01') 0.7272727272727273\n",
      "(1)-->Score syn-syn2\n",
      "Synset('bicycle.n.01') Synset('bicycle.n.01') 1.0\n",
      "(2)-->Score syn2-syn\n",
      "Synset('bicycle.n.01') Synset('bicycle.n.01') 1.0\n",
      "(2)-->Score syn2-syn\n",
      "Synset('bicycle.v.01') Synset('bicycle.n.01') 0.13333333333333333\n",
      "(1)-->Score syn-syn2\n",
      "Synset('bicycle.v.01') Synset('motorcycle.n.01') 0.11764705882352941\n",
      "(1)-->Score syn-syn2\n",
      "Synset('bicycle.v.01') Synset('bicycle.n.01') 0.13333333333333333\n",
      "(1)-->Score syn-syn2\n",
      "Synset('bicycle.v.01') Synset('bicycle.v.01') 1.0\n",
      "(2)-->Score syn2-syn\n",
      "Synset('bicycle.v.01') Synset('bicycle.v.01') 1.0\n"
     ]
    }
   ],
   "source": [
    "# we get all the sentences \n",
    "# strip punctuation marks \n",
    "# extract the feature names (words)\n",
    "# convert to dense -> list \n",
    "# check for the delta \n",
    "# are they \"equal\"? |token1| == |token2|\n",
    "# calculate similarity of the top synsets (token1 to token2) and (token2 to token1)\n",
    "# keep the similarity values for each of the sentences ! --> Feature for synset similarity!! \n",
    "\n",
    "# We only use the TF-IDF to look at the synset similarity measure of the two senses (sense1 and sense2)\n",
    "vec = TfidfVectorizer()\n",
    "resp = vec.fit_transform([sample_sentence, sample_sentence2])\n",
    "feature_names = vec.get_feature_names()\n",
    "\n",
    "dense_list = resp.todense().tolist()\n",
    "\n",
    "# the sentences we used\n",
    "indices = ['SentA','SentB']\n",
    "# final df with the TFIDF\n",
    "df = pd.DataFrame(dense_list, columns=feature_names).T\n",
    "\n",
    "\n",
    "df['delta'] = df[0] - df[1]\n",
    "\n",
    "\n",
    "#df.head()\n",
    "kk = min(df.delta)\n",
    "ki = max(df.delta)\n",
    "\n",
    "abs(kk) == abs(ki)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "word1 = df[df['delta']>0].index[0]\n",
    "word2 = df[df['delta']<0].index[0]\n",
    "\n",
    "#synset sim \n",
    "syn1 = wordnet.synsets(word1)\n",
    "syn2 = wordnet.synsets(word2)\n",
    "\n",
    "for syns in syn1:\n",
    "    for syns2 in syn2:\n",
    "        #print(syns, syns2)\n",
    "        score = syns.wup_similarity(syns2)\n",
    "        score2 = syns2.wup_similarity(syns)\n",
    "        if (score != 0) and score is not None:\n",
    "            print(\"(1)-->Score syn-syn2\")\n",
    "            print(syns, syns2, score)\n",
    "        if (score2 != 0) and score2 is not None:\n",
    "            print(\"(2)-->Score syn2-syn\")\n",
    "            print(syns2, syns, score2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'man', 'is', 'riding', 'a', 'bicycle']\n",
      "['a', 'man', 'is', 'riding', 'a', 'bike']\n",
      "['man', 'riding', 'bicycle']\n",
      "['man', 'riding', 'bike']\n",
      "[('man', 'NN'), ('riding', 'VBG'), ('bicycle', 'NN')]\n",
      "[('man', 'NN'), ('riding', 'VBG'), ('bike', 'NN')]\n",
      "['man', 'rid', 'bicycle']\n",
      "['man', 'rid', 'bike']\n"
     ]
    }
   ],
   "source": [
    "check_sent(sample_sentence, sample_sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Synset('bicycle.n.01'), Synset('bicycle.v.01')],\n",
       " [Synset('motorcycle.n.01'), Synset('bicycle.n.01'), Synset('bicycle.v.01')])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1)-->Score syn-syn2\n",
      "Synset('bicycle.n.01') Synset('motorcycle.n.01') 0.7272727272727273\n",
      "(2)-->Score syn2-syn\n",
      "Synset('motorcycle.n.01') Synset('bicycle.n.01') 0.7272727272727273\n",
      "(1)-->Score syn-syn2\n",
      "Synset('bicycle.n.01') Synset('bicycle.n.01') 1.0\n",
      "(2)-->Score syn2-syn\n",
      "Synset('bicycle.n.01') Synset('bicycle.n.01') 1.0\n",
      "(2)-->Score syn2-syn\n",
      "Synset('bicycle.v.01') Synset('bicycle.n.01') 0.13333333333333333\n",
      "(1)-->Score syn-syn2\n",
      "Synset('bicycle.v.01') Synset('motorcycle.n.01') 0.11764705882352941\n",
      "(1)-->Score syn-syn2\n",
      "Synset('bicycle.v.01') Synset('bicycle.n.01') 0.13333333333333333\n",
      "(1)-->Score syn-syn2\n",
      "Synset('bicycle.v.01') Synset('bicycle.v.01') 1.0\n",
      "(2)-->Score syn2-syn\n",
      "Synset('bicycle.v.01') Synset('bicycle.v.01') 1.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bicycle</th>\n",
       "      <td>0.630099</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bike</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.630099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.448321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.448321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>riding</th>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.448321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1\n",
       "bicycle  0.630099  0.000000\n",
       "bike     0.000000  0.630099\n",
       "is       0.448321  0.448321\n",
       "man      0.448321  0.448321\n",
       "riding   0.448321  0.448321"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2234"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to find the characters of all the sentences \n",
    "toked_sent_all = [nltk.word_tokenize(x) for x in all_sentences]\n",
    "#toked_sent_all = sum(toked_sent_all, [])\n",
    "#toked_sent_all_set = set(toked_sent_all)\n",
    "len(toked_sent_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7b0227df9456bca28f69aef5d39629306c490f3d5d6e0ff9d2fd6f7d7f6a539"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
