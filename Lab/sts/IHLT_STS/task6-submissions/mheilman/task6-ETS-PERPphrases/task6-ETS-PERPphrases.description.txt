========================

= TEAM =

Michael Heilman and Nitin Madnani
Educational Testing Service

= DESCRIPTION OF RUN =

We used the PERP system described below.  Separate models were trained for each task.  For the surprise OnWN task, we used the model trained on the MSRpar training data.  For the SMTnews task, we used the model trained on the SMTeuroparl training data.  For this run, we used a preliminary adaptation of TERp's pivot-based phrasal paraphrase edits, though that part of the code is still under development.  (For learning with the Perceptron, we set the learning rate to 0.01 and the number of iterations to 200.)

= TOOLS USED =

* Lemmatizer (Porter stemmer)
* Lexical Substitution
* Knowledge-based similarity
* Word-to-word Levenshtein (edit) distance
* TERp MT evaluation metric
* Pivot-based paraphrase database

= RESOURCES USED =

* Monolingual corpora (Gigaword NYT stories for word frequencies)
* Multilingual corpora (GALE Chinese corpora for extracting the pivot-based paraphrase database)
* WordNet

= METHOD TO COMBINE TOOLS AND RESOURCES =

The Paraphrase Edit Rate Perceptron (PERP) system is an extended version of the TERp machine translation metric.  PERP uses a discriminative feature-based learner based on the Averaged Perceptron instead of TERp's heuristic search routine.  We also implemented support for overlapping features, and extended the feature set from TERp to include various features for specific types of insertions, deletions, and substitutions (e.g., insertion of pronouns, deletion of punctuation, substitution of different numbers).

= COMMENTS =

