
Please include a description of your submission, following the format
below. 

Please don't modify lines starting with = unless instructed to do so.

Please delete lines starting with [. All lines starting with [ will be
ignored.

========================

= TEAM =

Sneha Jha 
Hansen Andrew Scwartz 
Lyle Ungar
University of Pennsylvania

= DESCRIPTION OF RUN =

We took an approach of combining a series of similarity metrics occuring at various levels depth, from surface similarity (word comparisons) to similarity in word / sentence meaning. Furthermore, we used both knowledge-based similarity metrics as well as less supervised statistical approaches such as Socher's sentence vectors or Ungar and Foster's word eigenvectors. The goal was to give our system many sources of similarity, and let the regression over the training data decide which similarity measures were most important. 
In this run, we ran linear regression over all features except the similarity between word eigen vectors.

= TOOLS USED =

* Part of speech tagger
* Lemmatizer
* Knowledge-based similarity
* Socher's sentence vectors (which uses a Syntactic Parser)
* Ungar & Foster's spectral methods for capturing word meaning

= RESOURCES USED =

* Monolingual corpora
* WordNet

= METHOD TO COMBINE TOOLS AND RESOURCES =

[Please summarize the method to combine the tools and resources, mentioning whether it's heuristic, or uses machine learning, etc.]

Our overall method used machine learning (regression) to combine similarity scores of various text-pair metrics (features to the regression). All resources were used within the specific methods to generate text similarity feature scores. For example, Ungar's spectral methods for capture word meaning used monolingual corpora, while the knowledge-based similarity approaches we used, utilized WordNet. 


= COMMENTS =

[Please include any comment you might have to improve the task in the future]

The ratings in the training data seemed subjectively inconsistent. I would suggest looking into more approaches to generate similarity ratings, perhaps being a little less direct.  For example, "On a scale of 0 to 5, how strongly do you feel that both sentence communicate the same information [in a similar tone / manner]." Directly being asked to rate similarity might result in raters relying on surface features rather than meaning. We also desired more dissimilar pairs of sentences -- at least more pairs that are dissimilar on the syntactic level, which may or may not be very similar in meaning. 

