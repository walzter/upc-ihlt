= TEAM =

[Please include affiliation and name of first author]
Dr. Harish Karnick
Professor,
Computer Science and Engineering,
IIT Kanpur,
Kanpur

= DESCRIPTION OF RUN =

[Please include a short paragraph describing your method for the run]

We use a 2-level "bag of words" approach to sentential similarity, where each sentence is broken down into tokens (words, named entities, adjectivally and numerically modified words), and the maximal bipartite match is found between two sentences. For multiword token similarity, a 2nd level maximal bipartite match is found which is normalized by the minimum of the two token lengths (heuristic). For word to word similarity in the 2nd level bipartite match, we use a combination of statistical similarity measure (DISCO - www.linguatools.de/disco/disco_en.html) which uses Wikipedia as the corpus, and a context capturing score which uses Jaccard similarity (heuristic). The 1st level maximal bipartite match score is normalized by the average of the two sentences' token length.

= TOOLS USED =

[Please keep those tools that you used, delete those you did not use, and add more lines if needed]

* Part of speech tagger
* Lemmatizer
* Syntax parser (Dependency Parser)
* Lexical Substitution
* Distributional similarity
* Knowledge-based similarity
* Time and date resolution
* Named Entity recognition

= RESOURCES USED =

[Please keep those resources that you used, delete those you did not use, and add more lines if needed]

* Monolingual corpora
* WordNet
* Wikipedia

= METHOD TO COMBINE TOOLS AND RESOURCES =

[Please summarize the method to combine the tools and resources, mentioning whether it's heuristic, or uses machine learning, etc.]

Every sentence is broken down into tokens (lemmatized words, named entities, adjectivally and numerically modified words). Sentence similarity is measured by finding the maximal bipartite match between the tokens of two sentences.
In the first level, multiword token similarity is measured using a 2nd level maximal bipartite match which is normalized by the minimum of the two token lengths (heuristic). In the 2nd level bipartite match, word to word similarity is measured by using a combination of statistical similarity measure (DISCO - www.linguatools.de/disco/disco_en.html), which uses distributional similarity, and a context capturing score which uses Jaccard similarity. The context of a word is captured by grouping words which are grammatically related to the word through dependencies (StanfordParser - Syntax Parser). The score is heuristic in nature.
The system is completely unsupervised.

= COMMENTS =

[Please include any comment you might have to improve the task in the future]
