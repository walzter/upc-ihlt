{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LAB3_IHLT_Session_3_Andrey_Eric.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UHXw43OEivBW"},"source":["# Import dependencies\n"]},{"cell_type":"code","metadata":{"id":"-IBUbpGijyb0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3VziSpFZzrF","executionInfo":{"status":"ok","timestamp":1634855140265,"user_tz":-120,"elapsed":21117,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}},"outputId":"3d4dbc6a-4a41-4691-e0c8-c34b4ea849e2"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"_DX_kXK_aDXt","executionInfo":{"status":"ok","timestamp":1634855175122,"user_tz":-120,"elapsed":526,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}}},"source":["PATH_INPUT = '/content/drive/MyDrive/Notebooks/IHLT/Labs/test-gold/STS.input.SMTeuroparl.txt'\n","PATH_GS = '/content/drive/MyDrive/Notebooks/IHLT/Labs/test-gold/STS.gs.SMTeuroparl.txt'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"bZ8Gt9loZa5j","executionInfo":{"status":"ok","timestamp":1634855177032,"user_tz":-120,"elapsed":1396,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}}},"source":["import nltk\n","import nltk\n","from scipy.stats import pearsonr\n","import pandas as pd\n","from nltk.metrics import jaccard_distance\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1VgYjPpsbzjW","executionInfo":{"status":"ok","timestamp":1634855178746,"user_tz":-120,"elapsed":1064,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}},"outputId":"c4183a01-0e81-4160-ac20-70291e1301e5"},"source":["nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"d7Tbt7-jjCFw"},"source":["# Load file"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":260},"id":"f3pwyapwabTl","executionInfo":{"status":"ok","timestamp":1634855183138,"user_tz":-120,"elapsed":2550,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}},"outputId":"78e672ca-a5eb-4f9c-a5a2-fae8c47686fb"},"source":["dt = pd.read_csv(PATH_INPUT,sep='\\t',header=None)\n","dt = dt.rename(columns={0: 'Set_A', 1: 'Set_B'})\n","dt['gs'] = pd.read_csv(PATH_GS,sep='\\t',header=None)\n","dt.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Set_A</th>\n","      <th>Set_B</th>\n","      <th>gs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The leaders have now been given a new chance a...</td>\n","      <td>The leaders benefit aujourd' hui of a new luck...</td>\n","      <td>4.50</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Amendment No 7 proposes certain changes in the...</td>\n","      <td>Amendment No 7 is proposing certain changes in...</td>\n","      <td>5.00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Let me remind you that our allies include ferv...</td>\n","      <td>I would like to remind you that among our alli...</td>\n","      <td>4.25</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The vote will take place today at 5.30 p.m.</td>\n","      <td>The vote will take place at 5.30pm</td>\n","      <td>4.50</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The fishermen are inactive, tired and disappoi...</td>\n","      <td>The fishermen are inactive, tired and disappoi...</td>\n","      <td>5.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               Set_A  ...    gs\n","0  The leaders have now been given a new chance a...  ...  4.50\n","1  Amendment No 7 proposes certain changes in the...  ...  5.00\n","2  Let me remind you that our allies include ferv...  ...  4.25\n","3        The vote will take place today at 5.30 p.m.  ...  4.50\n","4  The fishermen are inactive, tired and disappoi...  ...  5.00\n","\n","[5 rows x 3 columns]"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"_dIn85DDjGpC"},"source":["# Process data"]},{"cell_type":"code","metadata":{"id":"VPGOuCwMcR7Q","executionInfo":{"status":"ok","timestamp":1634855183977,"user_tz":-120,"elapsed":851,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}}},"source":["dt['Set_A'] = dt['Set_A'].apply(lambda x: nltk.sent_tokenize(x))\n","dt['Set_A'] = dt['Set_A'].apply(lambda x: nltk.word_tokenize(x[0]))\n","\n","dt['Set_B'] = dt['Set_B'].apply(lambda x: nltk.sent_tokenize(x))\n","dt['Set_B'] = dt['Set_B'].apply(lambda x: nltk.word_tokenize(x[0]))\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pvI6gv-5cULL","executionInfo":{"status":"ok","timestamp":1634855183979,"user_tz":-120,"elapsed":32,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}},"outputId":"7a5aa16e-c4c3-4163-cfc7-e82a7dca2ad4"},"source":["print(dt['Set_A'][0])"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["['The', 'leaders', 'have', 'now', 'been', 'given', 'a', 'new', 'chance', 'and', 'let', 'us', 'hope', 'they', 'seize', 'it', '.']\n"]}]},{"cell_type":"code","metadata":{"id":"VgBvBVhRbG3B","executionInfo":{"status":"ok","timestamp":1634855183981,"user_tz":-120,"elapsed":31,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}}},"source":["dt['Set_A'] = dt['Set_A'].apply(lambda x: nltk.pos_tag(x))\n","dt['Set_B'] = dt['Set_B'].apply(lambda x: nltk.pos_tag(x))\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v8_n7z8vrN2z","executionInfo":{"status":"ok","timestamp":1634855183982,"user_tz":-120,"elapsed":29,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}},"outputId":"ee0ec169-ec42-40de-a17e-53a2bd1e6d95"},"source":["dt['Set_A'][0]"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('The', 'DT'),\n"," ('leaders', 'NNS'),\n"," ('have', 'VBP'),\n"," ('now', 'RB'),\n"," ('been', 'VBN'),\n"," ('given', 'VBN'),\n"," ('a', 'DT'),\n"," ('new', 'JJ'),\n"," ('chance', 'NN'),\n"," ('and', 'CC'),\n"," ('let', 'VB'),\n"," ('us', 'PRP'),\n"," ('hope', 'VB'),\n"," ('they', 'PRP'),\n"," ('seize', 'VB'),\n"," ('it', 'PRP'),\n"," ('.', '.')]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"nHvt8_oQcM7O","executionInfo":{"status":"ok","timestamp":1634855183983,"user_tz":-120,"elapsed":15,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}}},"source":["wnl = nltk.stem.WordNetLemmatizer()\n","def lemmatize(p):\n","    if p[1][0] in {'N','V'}:\n","        return wnl.lemmatize(p[0].lower(), pos=p[1][0].lower())\n","    return p[0]"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dj7xJ5-WcXEt","executionInfo":{"status":"ok","timestamp":1634855185582,"user_tz":-120,"elapsed":1613,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}}},"source":["dt['lemma_SetA'] = dt['Set_A'].apply(lambda x: [lemmatize(pair) for pair in x])\n","dt['lemma_SetB'] = dt['Set_B'].apply(lambda x: [lemmatize(pair) for pair in x])\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"NleFmfFShqBt","executionInfo":{"status":"ok","timestamp":1634855185584,"user_tz":-120,"elapsed":48,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}}},"source":["dt['lemma_SetA'] = dt['lemma_SetA'].apply(set)\n","dt['lemma_SetB'] = dt['lemma_SetB'].apply(set)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxecSeVbfKm7","executionInfo":{"status":"ok","timestamp":1634855185585,"user_tz":-120,"elapsed":41,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}},"outputId":"9889efc4-8ee3-45b1-e27d-93c4cd87c61f"},"source":["dt['lemma_SetA']"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0      {a, now, and, it, ., The, they, let, us, be, g...\n","1      {certain, amendment, to, ., reference, 7, in, ...\n","2      {you, include, this, ally, remind, ., that, le...\n","3      {vote, take, will, ., The, p.m, place, 5.30, a...\n","4      {,, tired, ., The, fisherman, disappointed, be...\n","                             ...                        \n","454    {to, ., the, It, population, integration, our,...\n","455    {vote, take, will, ., The, p.m, place, 5.30, a...\n","456    {272, a, to, article, this, ., within, Neither...\n","457    {you, include, this, ally, remind, ., that, le...\n","458    {representative, citizen, pontificate, ., abou...\n","Name: lemma_SetA, Length: 459, dtype: object"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":610},"id":"TU2r3bLghd-N","executionInfo":{"status":"ok","timestamp":1634855185586,"user_tz":-120,"elapsed":38,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}},"outputId":"e82787ae-fd59-402d-f871-dbea5ec3d3c9"},"source":["dt['jacc_distance'] = ''\n","for index in dt.index:\n","  dt['jacc_distance'][index] = 1- jaccard_distance(dt['lemma_SetA'][index], dt['lemma_SetB'][index])\n","dt.head()"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Set_A</th>\n","      <th>Set_B</th>\n","      <th>gs</th>\n","      <th>lemma_SetA</th>\n","      <th>lemma_SetB</th>\n","      <th>jacc_distance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[(The, DT), (leaders, NNS), (have, VBP), (now,...</td>\n","      <td>[(The, DT), (leaders, NNS), (benefit, VBP), (a...</td>\n","      <td>4.50</td>\n","      <td>{a, now, and, it, ., The, they, let, us, be, g...</td>\n","      <td>{'s, ', therefore, a, and, seize, it, ., The, ...</td>\n","      <td>0.346154</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[(Amendment, NNP), (No, NNP), (7, CD), (propos...</td>\n","      <td>[(Amendment, NNP), (No, NNP), (7, CD), (is, VB...</td>\n","      <td>5.00</td>\n","      <td>{certain, amendment, to, ., reference, 7, in, ...</td>\n","      <td>{certain, amendment, to, ., reference, 7, in, ...</td>\n","      <td>0.923077</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[(Let, VB), (me, PRP), (remind, VB), (you, PRP...</td>\n","      <td>[(I, PRP), (would, MD), (like, VB), (to, TO), ...</td>\n","      <td>4.25</td>\n","      <td>{you, include, this, ally, remind, ., that, le...</td>\n","      <td>{there, among, would, ,, to, you, this, ally, ...</td>\n","      <td>0.391304</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[(The, DT), (vote, NN), (will, MD), (take, VB)...</td>\n","      <td>[(The, DT), (vote, NN), (will, MD), (take, VB)...</td>\n","      <td>4.50</td>\n","      <td>{vote, take, will, ., The, p.m, place, 5.30, a...</td>\n","      <td>{5.30pm, vote, take, will, The, place, at}</td>\n","      <td>0.545455</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[(The, DT), (fishermen, NNS), (are, VBP), (ina...</td>\n","      <td>[(The, DT), (fishermen, NNS), (are, VBP), (ina...</td>\n","      <td>5.00</td>\n","      <td>{,, tired, ., The, fisherman, disappointed, be...</td>\n","      <td>{,, tired, ., The, fisherman, disappointed, be...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               Set_A  ... jacc_distance\n","0  [(The, DT), (leaders, NNS), (have, VBP), (now,...  ...      0.346154\n","1  [(Amendment, NNP), (No, NNP), (7, CD), (propos...  ...      0.923077\n","2  [(Let, VB), (me, PRP), (remind, VB), (you, PRP...  ...      0.391304\n","3  [(The, DT), (vote, NN), (will, MD), (take, VB)...  ...      0.545455\n","4  [(The, DT), (fishermen, NNS), (are, VBP), (ina...  ...             1\n","\n","[5 rows x 6 columns]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z4s2F3jndJqC","executionInfo":{"status":"ok","timestamp":1634855185587,"user_tz":-120,"elapsed":25,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}},"outputId":"e26f85ed-3d50-4c8a-9245-164c2cf0be83"},"source":["pearsonr(dt['gs'], dt['jacc_distance'])[0]"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4569107458417673"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"O86XGS3r9D4n","executionInfo":{"status":"ok","timestamp":1634855186165,"user_tz":-120,"elapsed":596,"user":{"displayName":"Eric Walzthöny Kreutzberg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00568180445617102615"}}},"source":["PATH = '/content/drive/MyDrive/Notebooks/IHLT/Labs/lab6_data'\n","dt['jacc_distance'].to_csv(f\"{PATH}/Jaccard_Session3.csv\")"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rDeTuunH9gp9"},"source":["**Discussion**\n","Lemmatization is converts the word to its base form. Additionally, lemmatization takes into account the context and then converts the word to its \"_meaningful base form\" (Prabhakaran, Selva)_. In comparisson to work tokenization or sentence tokenization, where there we remove the stop words, and punctuations. What can happen is that the last characters of a word with a punctuation mark can be removed (i.e. isn't, it's,we've, etc..) especially where the context can be lost. On top of that, this causes a word to be in its incorrect form, this is an incorrect spelling thus a spelling error which can be propagated forward in the model or project that one is working on if it isn't taken into account. Quick note, that lemmatization would identify the corect base form of specific words: \n","\n","\"Caring\" --> Lemmatization --> \"Care\"\n","\n","(Prabhakaran, Selva) [Source](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)\n","\n","\n","During the past labs, we've seen the different methods in order to get the jaccard similarity of pairs of sentences. In lemmatization we can get the POS (Part of Speech), which is based off its definition and context. Whilst word tokenization can reduce the \"understanding\" of the corpus by removing characters from our base word. \n","\n","**Conclusion**\n","\n","In summary, the process of lemmatization is the one that takes into account the context and definition of the word by associating the POS. Taking this into consideration, it is most likely that we use lemmatization in order to analyze different corpus for NLP. \n","\n","In order to perform better for any pairs of texts, one would have to understand the meaning of the corpus as a whole, thus lemmatization would be a better choice. In order to make a better analysis of the text we would have to \"complete\" the workflow: \n","\n","Corpus --> Tokenization --> Pre-Processing (Stop-words --> Stemming & Lemmatization) --> Feature Engineering (Bag of Word, TF-IDF, Word Embedding) --> Model Build --> Model Evaluation.\n","\n","If we follow that path then what we can add is the following: \n","- Sentiment Analysis \n","- Bag of Words --> Document-Term Matrix (DTM)\n","- Classification (TF-IDF)\n","\n","_Sentiment Analysis_ --> This would be added in order to understand how negative or positive a given sentence is, because the words might be similar but what the text is trying to convey can vary depending how the context and definition of the word is used. \n","\n","_Bag of Words_ --> is a way to extract features from a document, which converts text into the \"matrix of occurence of words in a documents\"_(Avinash Navlani_) [Source](https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk), basically whether given words appeared in a document or not. \n","\n","_Classification_ --> Here we'd use TF-IDF, since it measure the amount of information a given word provides accross a document. \n","\n","To wrap it all up: \n","1. We'd use lemmatization as it contains more information such as context and definition of a word. \n","2. If we'd want to improve the analysis of pairs of text, then we could add sentiment analysis, BoW, or classification with a TF-IDF method. From there build a model and then evaluate the model. "]},{"cell_type":"code","metadata":{"id":"r6YayQsU9kyk"},"source":[""],"execution_count":null,"outputs":[]}]}