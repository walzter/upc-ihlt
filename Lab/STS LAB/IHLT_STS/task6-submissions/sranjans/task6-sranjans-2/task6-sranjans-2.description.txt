= TEAM =

[Please include affiliation and name of first author]
Dr. Harish Karnick
Professor,
Computer Science and Engineering,
IIT Kanpur,
Kanpur

= DESCRIPTION OF RUN =

[Please include a short paragraph describing your method for the run]

We use a 2-level "bag of words" approach to sentential similarity, where each sentence is broken down into tokens (words and named entities), and the maximal bipartite match is found between two sentences. For multiword token similarity, a 2nd level maximal bipartite match is found which is normalized by the minimum of the two token lengths (heuristic). For word to word similarity in the 2nd level bipartite match, we use a statistical similarity measure (DISCO - www.linguatools.de/disco/disco_en.html) which uses Wikipedia as the corpus. The 1st level maximal bipartite match score is normalized by the average of the two sentences' token length. For the MSRvideo dataset, we use a combination of DISCO similarity and WordNet similarity.

= TOOLS USED =

[Please keep those tools that you used, delete those you did not use, and add more lines if needed]

* Part of speech tagger
* Lemmatizer
* Word Sense Disambiguation
* Lexical Substitution
* Distributional similarity
* Knowledge-based similarity
* Time and date resolution
* Named Entity recognition

= RESOURCES USED =

[Please keep those resources that you used, delete those you did not use, and add more lines if needed]

* Monolingual corpora
* WordNet
* Wikipedia

= METHOD TO COMBINE TOOLS AND RESOURCES =

[Please summarize the method to combine the tools and resources, mentioning whether it's heuristic, or uses machine learning, etc.]

Every sentence is broken down into tokens (lemmatized words and named entities). Sentence similarity is measured by finding the maximal bipartite match between the tokens of two sentences.
In the first level, multiword token similarity is measured using a 2nd level maximal bipartite match which is normalized by the minimum of the two token lengths (heuristic). In the 2nd level bipartite match, word to word similarity is measured by using a statistical similarity measure (DISCO - www.linguatools.de/disco/disco_en.html), which uses distributional similarity. For the MSRvideo dataset, a combination of DISCO and a Knowledge-based similarity measure (WordNet similarity - Lin similarity) is used. For the WordNet based similarity, a Word Sense Disambiguation algorithm is used (Lesk Algorithm). The score is heuristic in nature.
The system is completely unsupervised.

= COMMENTS =

[Please include any comment you might have to improve the task in the future]
