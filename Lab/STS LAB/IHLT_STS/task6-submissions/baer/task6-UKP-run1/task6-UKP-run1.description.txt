========================

= TEAM =

Daniel Bär, UKP Lab, Technische Universität Darmstadt


= DESCRIPTION OF RUN =

We use a linear regression machine learning classifier to combine a multitude of text similarity measures for detecting the degree of semantic equivalence between the given sentence pairs. Our feature set comprises measures which operate on the lexical level (string sequences, n-gram models) as well as the semantic level (based on lexical-semantic resources such as WordNet).

For this run, we trained our model separately for each of the three known datasets on the given training data. The surprise datasets were trained on a joint dataset comprising all three known datasets.


= TOOLS USED =

* Part of speech tagger
* Lemmatizer
* Stopword filter
* Distributional similarity
* Knowledge-based similarity
* String similarity
* Textual Entailment


= RESOURCES USED =

* Monolingual corpora
* WordNet
* Wiktionary
* Wikipedia
* Distributional thesaurus
* Statistical machine translation (MOSES on Europarl)
* Textual entailment system
* TWSI lexical substitution system

= METHOD TO COMBINE TOOLS AND RESOURCES =

Our natural language processing framework is based on Apache UIMA. First, we computed the individual similarity scores with all the available text similarity measures in several configurations, resulting 300+ features. Besides measures on the lexical level, we used, for example, semantic similarity measures such as Explicit Semantic Analysis (on WordNet, Wiktionary, and Wikipedia) as well as aggregated word similarity measures on WordNet. We then used the linear regression machine learning classifier from the WEKA toolkit to combine these similarity features. During our experiments, we were able to cut the final feature set down to 19 distinct features (for this run).


= COMMENTS =


