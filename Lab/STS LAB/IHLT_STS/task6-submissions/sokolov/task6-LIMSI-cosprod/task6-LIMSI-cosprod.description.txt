========================

= TEAM =

LIMSI-CNRS
Artem Sokolov

= DESCRIPTION OF RUN =

A non-linear variant of the cosine similarity measure between context vectors.

= TOOLS USED =

* Stemmer
* Distributional similarity
* Machine Learning: Ranking by Boosting, Gradient Boosting

= RESOURCES USED =

* Monolingual corpora: GigaWord

= METHOD TO COMBINE TOOLS AND RESOURCES =

First, sparse context vectors were constructed with context width 4 and dimension 10000 on the "stemmed" and low-cased English GigaWord corpus.
The degree of initial sparseness was set to 1/1000 (ternary elements).
In this way an embedding of indicator vectors of context sets into Euclidian space was obtained. 
The construction of the embedding is analogous to Johnson-Lindestrauss embeddings:
each new feature is a projection on a random direction activating or disactivating certain words from the dictionary.

The vectors for test sentences were obtained by summing the context vectors for each individual word.
In this run we proposed a method of selecting informative random directions of the initial projective embedding based on their contribution to the generalized cosine similarity between these sentence-vectors.

Instead of classical cosine similarity between sentence-vectors we learned a weighted and non-normalized version of it:
\[
cos'(v_1,v_2) = \sum_i H^i(v^i_1\cdot v^i_2),
\]
where $H^i$ are non-linear functions that minimize the pair-wise ranking loss on the training data set.
In this way the "random" features of the context vectors are selected according to their contribution to the similarity being learned 
and non-linearly tranformed to better fit this similarity.

The pair-wise loss is defined as:
\[
L = \sum_{(s_1,s_2,s'_1,s'_2): t(s_1,s_2) > t(s'_1,s'_2)} D(s_1,s_2,s'_1,s'_2) [cos'(v(s'_1),v(s'_2)) > cos'(v(s_1),v(s_2))],
\]
where $(s_1,s_2)$ and $(s'_1,s'_2)$ are a two pairs of sentences from the training set.
The sum runs over sentence pairs such that the first pair is ranked higher than the second.
The weighting multiplier $D$ was set to be equal to a difference between averaged judges labels.

The loss is non-convex and non-differentiable, so a convex upper bound to this non-convex loss 
was optimized by a well-known boosting technique RankBoost (or AdaBoost for ranking).

Number of boosting iterations and number of random directions were found by cross-validation on 5 folds.

Finally, a single model was done for the whole training set.

= COMMENTS =

1) Only the averaged similarity scores were provided, thus obfuscating the class decisions made by individual judges.
Having non-averaged class labels would have allowed applying a wider range of machine learning techniques.

2) The dataset contained a very limited subset of sentences that appear in more than one pair. 
It would be interesting to apply metric learning techniques based on equivalence constraints -- for this a sentence ideally should have specified sentences to which it is "close" as well a sentences to which it is distant.

3) It might be interesting to have an automatic leaderboard with results calculated on a subset of the test dataset. 
It should be visible to all contestants to compare one's progress to the progress of other teams. 
This also allows avoiding formatting errors, that are impossible to spot with a single submission allowed. 
For an example of such a platform see, e.g, Kaggle.com.
