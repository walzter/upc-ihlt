========================

= TEAM =

Michael Heilman and Nitin Madnani
Educational Testing Service

= DESCRIPTION OF RUN =

We used the PERP system described below.  Separate models were trained for each task.  For the surprise OnWN task, we used the model trained on the MSRpar training data.  For the SMTnews task, we used the model trained on the SMTeuroparl training data.  For this run, we did not use the pivot-based phrasal paraphrase edits in the original TERp.  (For learning with the Perceptron, we set the learning rate to 0.01 and the number of iterations to 200.)

= TOOLS USED =

* Lemmatizer (Porter stemmer)
* Lexical Substitution
* Knowledge-based similarity
* Word-to-word Levenshtein (edit) distance
* TERp MT evaluation metric

= RESOURCES USED =

* Monolingual corpora (Gigaword NYT stories for word frequencies)
* WordNet

= METHOD TO COMBINE TOOLS AND RESOURCES =

The Paraphrase Edit Rate Perceptron (PERP) system is an extended version of the TERp machine translation metric.  PERP uses a discriminative feature-based learner based on the Averaged Perceptron instead of TERp's heuristic search routine.  We also implemented support for overlapping features, and extended the feature set from TERp to include various features for specific types of insertions, deletions, and substitutions (e.g., insertion of pronouns, deletion of punctuation, substitution of different numbers).

= COMMENTS =

