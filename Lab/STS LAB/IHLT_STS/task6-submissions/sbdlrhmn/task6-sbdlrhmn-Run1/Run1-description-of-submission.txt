= TEAM =

Samir AbdelRahman, Graduate School of Library and Information Sciences, UIUC, USA

= DESCRIPTION OF RUN =

"Human Annotation Emulation":
Two sentences are considered semantically comparable, if there human interpretations have similar meaningful contents. Human interpretations are based on how much the contents contain shared or different valuable information. In our proposed solution, we aim to heuristically assess the values of their shared and missing information to calculate their semantic similarity degree; all our heuristics and thresholds are calculated by trial and error from the training data set words. 
In each pair, we calculate the word score (Ws) that presents its maximum semantic relatedness to the other sentence words.  Each Ws, ranging from 0 to 1, equals to the maximum of:
        - Values obtained from the comparison with the other sentence words—pairwise ( [1] Equation 1,2 ) and [2]  “All senses are used”
        - The value obtained from the comparison of its (Subject-VP) with the other sentences phrases (Subject-VP).  We use [3] to find the best sense of the word within window of words to compute the best word score.
If Ws is:
       - 0 <= Ws < 0.3, then the word is considered not related to any other sentence words.
       - 0.3 <= Ws < 0.85, then the word is weakly considered related to the other sentence. 
       - Ws >= 0.85, then the word is considered strongly related to the other sentence. 

Then, we calculate the sentence semantic score (Ss) as the average of its word scores.  We consider the information important if it is the sentence’ main verbs, head nouns, named entities or  missing whole noun/verb phrases. However, all types of words are considered when the sentence semantic score is calculated as described above.  From the training data analysis, we have 7 classes (0), (0-1), (1), (2), (3), (4), (5) as follows
Branch (0): Two sentences are not related to each other if they haven’t semantically related verbs. For example, “the man smokes” and “the man cooks” as ‘smokes and cooks” may not semantically be equivalent. Also,   “The man shoots the thief” and “The man eats an apple” as “   “shoots an apple” and “eats the thief” are impossibly occurred in the texts. 
If not (0), we consider all strong Ws of both sentences and compute the minimum percentage of them (P), D =100-P, and calculate the semantic similarity (Sim )as follows:
    1-	Branch (5):    If 95 <= P <=1: 
            *if all D for words not in important information, the two sentences Sim = 5.00.
            *Else: Goto Branch (3).
    2-	Branch (4):  If 80 <= P < 95:
            *if all D for words not in important information:
                -If all D words in weak ranges, Sim = 4.00 + (Ss1+Ss2)/2
                -Else if: at least half D in weak ranges, Sim = 4.00 + average all S1,S2 weak values
                -Otherwise: Sim= 4.00 + average all S1,S2 not related values
            *Else: Goto Branch (3).
    3-	Branch (3):  If 50 <= P < 80:
            *If all D for words in important information, the two sentences Sim = 3.00
            *If all D for words not in important information, Goto Branch (2)
            *Otherwise:
               -If all D words in weak ranges, Sim = 3.00 + (Ss1+Ss2)/2
               - Else if: at least half D  in weak ranges, Sim = 3.00 + average all S1,S2 weak values
               -Otherwise: Sim= 3 + average all S1,S2 not related values
    4-	Branch (2):  If 20 <= P < 50:
               -If all D words in weak ranges, Sim =2.00 + (Ss1+Ss2)/2
               - Else if: at least half D in weak ranges, Sim = 2.00 + average all S1,S2 weak values
               -Otherwise: Sim= 2.00 + average all S1,S2 not related values
    5-	Branch (1):  If 0 < P < 20: (All of them are strong verbs)
               -If all D words in weak ranges, Sim =1.00 + (Ss1+Ss2)/2
               -Else if: at least half D  in weak ranges, Sim = 1.00 + average all S1,S2 weak values
               -Otherwise: Sim= 1.00 + average all S1,S2 not related values
    6-	Branch (0-1):  If 0 < P < 20: (not All of them are strong verbs)
               -If all D words in weak ranges, Sim =0.00 + (Ss1+Ss2)/2
               -Else if: at least half D  in weak ranges, Sim = 0.00 + average all S1,S2 weak values
               -Otherwise: Sim= 0.00 + average all S1,S2 not related values

Using the above scoring algorithm, we use the following steps as a main system algorithm steps:
    A-	We remove some stop words and symbols from the data such as ‘!’,’;’,…,‘a’,’an’,’the’,’days’,….
    B-	We use Stanford Parser [4] for ‘POS tagging’, ‘syntactic’, and ’dependencies’ constituents.
    C-	We use S1:(Subject-Verb-Object) and S2:(Subject-Verb-Object) as follows:
        -If S1(Verb) and S2(Verb) are at least weakly comparable, then start Branch (5) checks
        -Else:
            We hit TextRunner [5] to find scores of S1(Verb)-S2(Object) and S2(Verb)-  S1(Object). If both zeros, then they are in Branch (0)
    D-	We use Dan Roth Coreference s/w [6] to resolve correference and entities problems.
    E-	In our matching algorithm, we consider the following special cases:
        - Examples like ‘shot gun’ and ‘shotgun’
        - Numbers and numerical words ,i.g. one, …, hundreds,…  
    F-	If the sentence S1 word not in Wordnet Dictionary we take this word and all other sentence S2 words as pairwise arguments to TextRunner—If the score not equal zero, we approximately put this word score = 0.2,0.4,0.6,0.8 for <=10, <=100, <=1000, > 1000 scores
    G-	We have some other heuristics that we intend to explain in the paper.

We believe that the success of our algorithm is based on the accuracy of:
    1-	The calculation of the semantic similarity scores.
    2-	The tools we use in our experiments.
    3-	Some other future enhancements should be done.. “We explicitly mention them in the paper”.
    

References:
    [1] Xiao-Ying Liu,   Yi-Ming Zhou,   Ruo-Shi Zheng ,“ Measuring semantic similarity within sentences ”, Proceedings of the Seventh International Conference on Machine Learning and Cybernetics, Kunming, 12-15 July 2008
    [2] Satanjeev Banerjee, Ted Pedersen: An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet. CICLing 2002: 136-145
    [3] Blake,C., “The Role of Sentence Structure in Recognizing Textual Entailment. The Third Recognizing Textual Entailment Challenge, 2007
    [4] Stanford Parser. http://nlp.stanford.edu/software/lex-parser.shtml.
    [5] TextRunner ReVerb Search (Experimental) . http://openie.cs.washington.edu/.
    [6] Illinois Coreference Package.  http://cogcomp.cs.illinois.edu/page/software_view/Coref


= TOOLS USED =


* Part of speech tagger
* Lemmatizer
* Syntax parser
* Word Sense Disambiguation
* Distributional similarity
* Knowledge-based similarity
* Named Entity recognition
* Correference

= RESOURCES USED =


* WordNet
* Text Runner
* Illinois Coreference Package
* Python NLTK tool


= METHOD TO COMBINE TOOLS AND RESOURCES =

Heuristics Rule Based System. I describe all methods in the Desciption Section


= COMMENTS =


