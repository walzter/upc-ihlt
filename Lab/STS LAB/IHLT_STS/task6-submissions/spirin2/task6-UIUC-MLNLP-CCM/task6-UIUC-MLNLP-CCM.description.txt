
Please include a description of your submission, following the format
below. 

Please don't modify lines starting with = unless instructed to do so.

Please delete lines starting with [. All lines starting with [ will be
ignored.

========================

= TEAM =

Dan Roth, UIUC

= DESCRIPTION OF RUN =

The UIUC CCM team developed a method.
Except for in our LLM baseline model, we try to reduce similarity to entailment, treating one sentence as the text and one as the hypothesis (and vice versa -- this is why we average). To do alignment, therefore, we match each token of the hypothesis sentence with a single (maximal match) token of the text sentence, and normalize over the length of the hypothesis sentence. Most of our views treat 'alignment' this way, although we may match over constituents instead of individual words, and the scoring measure that assigns a value to a pair will vary.

Total number of features: 54

(1) Shallow parsing (chunk) (2 features)

(2) Combine the "Numerical Quantities" with "Dependency' parser.  (24 features)
Through the Dependecy parser, we can get follow edges.
    num(points, 20.96)
    num(points, 23.54)
If the source (like points) or target of both of two sentences are the same, we can compare the number (20.96 and 23.54). We also provided just number compare (not consider the Dependency paser).

(3) Dependency parser   (10 features)
If the edges from two sentences have the same label, then compare them, and get the maxmum score. Count the number of (both source and target are the same; just one of them are the same; both of them are different)

(5) Stanford Parser   (4 features)
For short sentence (such like MSRvid corpus), most of sentences are consisit of the stucture (S-->NP VP). So we compare the NP (the whole subject part) and VP (the whole predicate part) respectively between two sentences.

(6) POS (1 feature) 
Check the alignments between content words (nouns and verbs). Use proper nouns tags to discover topic similarity.

(7) Named Entity (2 features)

(8) SRL (7 features)
Predicates (verbs) are matched between hypothesis and text based on argument similariy (labeled accuracy, two-way), using a WNsim similarity between the arguments' labels as a partial count. Other features include sentence length ratio, fraction of words in common (two-way), and WNsim-based fraction of words in common (two-way).

(9) LLM (2 features)
Using LLM provided by CGS

(10) LLM (2 features)
Using LLM implemented with WNsim, stop words are filtered.

= TOOLS USED =

Part of speech tagger
Multiword expressions recognition
Syntax parser
Semantic Role Labeling
Named Entity recognition
Scoping
Dependency parsing
Textual entailment

= RESOURCES USED =

Monolingual corpora
Tables of paraphrases
Repositories for named-entities and acronyms
Wikipedia

= METHOD TO COMBINE TOOLS AND RESOURCES =

machine learning, based on the labeled data for existing similarity values. SVM, MaxEnt.

= COMMENTS =

[Please include any comment you might have to improve the task in the future]
